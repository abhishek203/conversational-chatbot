{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFOijuPbROUN7QMiEs936m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek203/conversational-chatbot/blob/master/Encoder_Decoder_model_for_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNvesS-JZjW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers.experimental.preprocessing as tokens\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrUMLPcdzt6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = \"/content/chatbot_data_small.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQDyT-izypN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = io.open(path_to_file,errors = 'ignore').read().strip().split('- - ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd4dsLnHz9CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_question = []\n",
        "raw_ans = []\n",
        "for i in range(len(data)):\n",
        "    pair = data[i].split('  - ')\n",
        "    for j in range(len(pair)-1):\n",
        "        raw_question.append(pair[0])\n",
        "        raw_ans.append(pair[j+1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VnTTJy0B0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0htXv10B0GEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_questions = []\n",
        "for question in raw_question:\n",
        "    clean_questions.append(clean_text(question))\n",
        "    \n",
        "clean_answers = []    \n",
        "for answer in raw_ans:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wfL5z7Q0Hls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = []\n",
        "ans = []\n",
        "pair =[]\n",
        "for i in range(len(clean_questions)):\n",
        "  question.append('SOS '+ clean_questions[i] +' EOS')\n",
        "  ans.append('SOS '+ clean_answers[i] +' EOS')\n",
        "  pair.append([question[i],ans[i]])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7DMM59knNeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<oov>')\n",
        "\n",
        "tokenizer.fit_on_texts(question)\n",
        "question_seq = tokenizer.texts_to_sequences(question)\n",
        "question_seq = tf.keras.preprocessing.sequence.pad_sequences(question_seq,padding='post')\n",
        "tokenizer.fit_on_texts(ans)\n",
        "ans_seq = tokenizer.texts_to_sequences(ans)\n",
        "ans_seq = tf.keras.preprocessing.sequence.pad_sequences(ans_seq, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDXCmNgnf464",
        "colab_type": "code",
        "outputId": "e8586342-9bcb-4a36-b55e-f009d63573b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(question_seq[0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_SZXO0Z1Okd",
        "colab_type": "code",
        "outputId": "f628a539-e1e9-4c99-c5f5-aac7dfe7d9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(ans_seq[0])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAvoMx_l1AHT",
        "colab_type": "code",
        "outputId": "ee48978b-c8a6-4e6e-c076-6018ab34de47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNE4c9q1NUxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((question_seq, ans_seq)).shuffle(1000)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2GHTx-WZozN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_input=2218,vocab_output=2218,BATCH_SIZE=32):\n",
        "    super(EncoderDecoder,self).__init__()\n",
        "    #Encoder\n",
        "    self.encoder_embedding = tf.keras.layers.Embedding(vocab_input,256)\n",
        "    self.encoder_lstm = tf.keras.layers.LSTM(512,return_sequences=True,return_state=True)\n",
        "\n",
        "\n",
        "    #Decoder\n",
        "    self.decoder_embedding = tf.keras.layers.Embedding(vocab_output,256)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_output)\n",
        "    self.decoder_lstm = tf.keras.layers.LSTM(512,return_sequences=True,return_state=True)\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self,data):\n",
        "    loss_val =0\n",
        "    input,targ = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      input = self.encoder_embedding(input)\n",
        "      enc_output,enc_h,enc_c = self.encoder_lstm(input)\n",
        "\n",
        "      dec_h = enc_h\n",
        "      dec_input = tf.expand_dims([tokenizer.word_index['sos']]*BATCH_SIZE,1)\n",
        "      predictions = []\n",
        "\n",
        "      for t in range(1,targ.shape[1]):\n",
        "        dec_input = self.decoder_embedding(dec_input)\n",
        "        dec_output,dec_h,dec_c = self.decoder_lstm(dec_input,initial_state=[dec_h,enc_c])\n",
        "        dec_output = tf.reshape(dec_output, (-1, dec_output.shape[2]))\n",
        "        predictions = self.fc(dec_output)\n",
        "\n",
        "        loss_val+=loss_fn(targ[:,t],predictions)\n",
        "\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "      gradients = tape.gradient(loss_val,self.trainable_variables)\n",
        "\n",
        "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "\n",
        "      self.compiled_metrics.update_state(targ,predictions)\n",
        "\n",
        "      return {m.name:m.result() for m in self.metrics}\n",
        "\n",
        "  def predict(self,sentence):\n",
        "    max_length_inp = 24\n",
        "    max_length_targ =44\n",
        "    \n",
        "\n",
        "    inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    inputs = self.encoder_embedding(inputs)\n",
        "    enc_out, enc_h,enc_c = self.encoder_lstm(inputs)\n",
        "\n",
        "    dec_h = enc_h\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['sos']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "      dec_input = self.decoder_embedding(dec_input)\n",
        "      dec_output,dec_h,dec_c = self.decoder_lstm(dec_input,initial_state=[dec_h,enc_c])\n",
        "      dec_output = tf.reshape(dec_output, (-1, dec_output.shape[2]))\n",
        "      predictions = self.fc(dec_output)\n",
        "      \n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "      result += tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "      if tokenizer.index_word[predicted_id] == 'eos':\n",
        "        return result\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result\n",
        "  @tf.function\n",
        "  def call(self,x):\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYCv3ex32LuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  \n",
        "  def loss_fn(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LdTnIveFbrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EncoderDecoder()\n",
        "model.compile(loss = loss_fn, optimizer = 'adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgboF8T0N4wX",
        "colab_type": "code",
        "outputId": "8c9611b7-ca89-442d-9e16-c61059030068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.fit(dataset,epochs=1)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 7s 223ms/step - accuracy: 0.1139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8971ef9898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hba528SNZmUU",
        "colab_type": "code",
        "outputId": "82f49f8b-3dee-493b-c009-fff6514f7cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.predict('hi')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i eos '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97V0CAyjZmbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model._set_inputs(inputs=tf.TensorSpec(\n",
        "    shape=[32,24], dtype=tf.dtypes.float32\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwh0HmLim9wC",
        "colab_type": "code",
        "outputId": "5881dde5-3774-4384-81af-49a2c02d08ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "open(\"chatbot_small.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    }
  ]
}